import gymnasium as gym
import numpy as np
import time
from stable_baselines3 import PPO
from cleaning_env import CleaningRobotEnv

def print_banner():
    """In banner ngáº§u cho chÆ°Æ¡ng trÃ¬nh"""
    print("\n" + "="*60)
    print("ğŸ¤– CLEANING ROBOT AI TRAINING SYSTEM ğŸ¤–")
    print("="*60)
    print("ğŸš€ Powered by Deep Reinforcement Learning")
    print("âš¡ Algorithm: Proximal Policy Optimization (PPO)")
    print("ğŸ¯ Mission: Train the Ultimate Cleaning Robot")
    print("="*60 + "\n")

def print_progress_bar(iteration, total, prefix='', suffix='', decimals=1, length=50, fill='â–ˆ'):
    """In thanh tiáº¿n trÃ¬nh ngáº§u"""
    percent = ("{0:." + str(decimals) + "f}").format(100 * (iteration / float(total)))
    filled_length = int(length * iteration // total)
    bar = fill * filled_length + '-' * (length - filled_length)
    print(f'\r{prefix} |{bar}| {percent}% {suffix}', end='\r')
    if iteration == total:
        print()

def train_episode(model, env, episode_num):
    """Huáº¥n luyá»‡n má»™t episode vÃ  tráº£ vá» reward"""
    obs = env.reset()
    episode_rewards = []
    steps = 0
    
    done = False
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, _ = env.step(action)
        episode_rewards.append(reward)
        steps += 1
    
    total_reward = sum(episode_rewards)
    print(f"ğŸ“ˆ Episode {episode_num:2d}: Reward = {total_reward:8.2f} | Steps = {steps:4d}")
    return total_reward

# Khá»Ÿi táº¡o há»‡ thá»‘ng
print_banner()
print("ğŸ”§ Initializing Training Environment...")
env = CleaningRobotEnv()
print("âœ… Environment loaded successfully!")

print("ğŸ§  Creating PPO Neural Network...")
model = PPO("MlpPolicy", env, verbose=0)
print("âœ… AI Model initialized!")

print("\nğŸ® Starting Training Sequence...")
print("-" * 60)

# Cáº¥u hÃ¬nh training
rewards = []
TIMESTEPS = 100000
TRAINING_ROUNDS = 10
STEPS_PER_ROUND = TIMESTEPS // TRAINING_ROUNDS

start_time = time.time()

for i in range(TRAINING_ROUNDS):
    round_start = time.time()
    
    # Hiá»ƒn thá»‹ thÃ´ng tin round
    print(f"\nğŸ”¥ TRAINING ROUND {i+1}/{TRAINING_ROUNDS}")
    print(f"â±ï¸  Target Steps: {STEPS_PER_ROUND:,}")
    
    # Training vá»›i progress bar
    print("ğŸš€ Learning in progress...")
    model.learn(total_timesteps=STEPS_PER_ROUND)
    
    # Test performance
    print("ğŸ¯ Testing robot performance...")
    episode_reward = train_episode(model, env, i+1)
    rewards.append(episode_reward)
    
    # Thá»‘ng kÃª round
    round_time = time.time() - round_start
    print(f"â° Round completed in {round_time:.1f}s")
    
    # Progress bar tá»•ng thá»ƒ
    print_progress_bar(i+1, TRAINING_ROUNDS, prefix='Overall Progress:', suffix='Complete', length=40)

# Thá»‘ng kÃª cuá»‘i cÃ¹ng
training_time = time.time() - start_time
print(f"\nğŸ‰ TRAINING COMPLETED!")
print("="*60)
print(f"â±ï¸  Total Training Time: {training_time:.1f} seconds")
print(f"ğŸ“Š Average Reward: {np.mean(rewards):.2f}")
print(f"ğŸ† Best Reward: {np.max(rewards):.2f}")
print(f"ğŸ“ˆ Improvement: {((rewards[-1] - rewards[0]) / abs(rewards[0]) * 100):.1f}%")
print("="*60)

# LÆ°u mÃ´ hÃ¬nh
print("\nğŸ’¾ Saving AI Model...")
model.save("ppo_cleaning_robot")
print("âœ… Model saved as 'ppo_cleaning_robot'")

# LÆ°u dá»¯ liá»‡u training
print("ğŸ“Š Saving training data...")
np.savetxt("training_rewards.txt", rewards)
print("âœ… Rewards saved to 'training_rewards.txt'")

print("\nğŸ¯ MISSION ACCOMPLISHED!")
print("ğŸ¤– Your cleaning robot is now ready to dominate the world of cleanliness!")
print("ğŸš€ Use visualize_learning.py to see the epic training journey!")
print("="*60 + "\n")
